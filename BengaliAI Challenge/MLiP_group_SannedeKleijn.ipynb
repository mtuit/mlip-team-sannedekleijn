{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IPUqUn25oYdm"
   },
   "source": [
    "# MLiP - Bengali.AI - Group Sanne de Kleijn\n",
    "\n",
    "## Libraries\n",
    "\n",
    "As a first step we need to import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CBGm4gN9oYdr"
   },
   "outputs": [],
   "source": [
    "# We use tensorflow's version of Keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TjP5_M7toYdy"
   },
   "source": [
    "## Loading the dataset\n",
    "For this notebook we use the Bengali.AI handwritten Grapheme data set from Kaggle \\[1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "QYSXg-5uoYd0",
    "outputId": "0dc2d728-562d-4c98-bead-0b948ea8da55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/.DS_Store\n",
      "Data/class_map.csv\n",
      "Data/sample_submission.csv\n",
      "Data/test.csv\n",
      "Data/test_image_data_0.parquet\n",
      "Data/test_image_data_1.parquet\n",
      "Data/test_image_data_2.parquet\n",
      "Data/test_image_data_3.parquet\n",
      "Data/train.csv\n",
      "Data/train_image_data_0.parquet\n",
      "Data/train_image_data_1.parquet\n",
      "Data/train_image_data_2.parquet\n",
      "Data/train_image_data_3.parquet\n",
      "train_df shape:  (200840, 5)\n",
      "test_df shape:  (36, 3)\n",
      "class_map_df shape:  (186, 3)\n",
      "sample_submission_df shape:  (36, 2)\n"
     ]
    }
   ],
   "source": [
    "# Check the data\n",
    "for dirname, _, filenames in os.walk('Data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Load the train data\n",
    "DATA_FOLDER = 'Data/'\n",
    "train_df = pd.read_csv(os.path.join(DATA_FOLDER, 'train.csv'))\n",
    "train_df.head()\n",
    "\n",
    "print('train_df shape: ', train_df.shape)\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))\n",
    "test_df.head()\n",
    "\n",
    "print('test_df shape: ', test_df.shape)\n",
    "\n",
    "# Load class map\n",
    "class_map_df = pd.read_csv(os.path.join(DATA_FOLDER, 'class_map.csv'))\n",
    "class_map_df.head()\n",
    "\n",
    "print('class_map_df shape: ', class_map_df.shape)\n",
    "\n",
    "# Load sample submissions\n",
    "sample_submission_df = pd.read_csv(os.path.join(DATA_FOLDER, 'sample_submission.csv'))\n",
    "sample_submission_df.head()\n",
    "\n",
    "print('sample_submission_df shape: ', sample_submission_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with only two files \\[1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read first file\n",
    "start_time = time.time()\n",
    "train_0_df = pd.read_parquet(os.path.join(DATA_FOLDER,'train_image_data_0.parquet'))\n",
    "print(f\"`train_image_data_0` read in {round(time.time()-start_time,2)} sec.\") \n",
    "\n",
    "print('train_0_df shape: ', train_0_df.shape)\n",
    "\n",
    "print(train_0_df.head())\n",
    "\n",
    "# Read second file\n",
    "start_time = time.time()\n",
    "train_1_df = pd.read_parquet(os.path.join(DATA_FOLDER,'train_image_data_1.parquet'))\n",
    "print(f\"`train_image_data_1` read in {round(time.time()-start_time,2)} sec.\")  \n",
    "\n",
    "print('train_0_df shape: ', train_0_df.shape)\n",
    "\n",
    "print(train_0_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read test files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with only one test file \\[1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read first test file\n",
    "start_time = time.time()\n",
    "test_0_df = pd.read_parquet(os.path.join(DATA_FOLDER,'test_image_data_0.parquet'))\n",
    "print(f\"`test_image_data_0` read in {round(time.time()-start_time,2)} sec.\")  \n",
    "\n",
    "print('train_0_df shape: ', train_0_df.shape)\n",
    "\n",
    "print(train_0_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check whether the distribution of the classes matches the kaggle dataset \\[1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train: unique grapheme roots: {train_df.grapheme_root.nunique()}\")\n",
    "print(f\"Train: unique vowel diacritics: {train_df.vowel_diacritic.nunique()}\")\n",
    "print(f\"Train: unique consonant diacritics: {train_df.consonant_diacritic.nunique()}\")\n",
    "print(f\"Train: total unique elements: {train_df.grapheme_root.nunique() + train_df.vowel_diacritic.nunique() + train_df.consonant_diacritic.nunique()}\")\n",
    "print(f\"Class map: unique elements: \\n{class_map_df.component_type.value_counts()}\")\n",
    "print(f\"Total combinations: {pd.DataFrame(train_df.groupby(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'])).shape[0]}\")\n",
    "      \n",
    "\n",
    "# TODO: asserts? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect grapheme images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to show a sample of size * size (ex: 5 x 5 = 25) handwritten graphemes \\[1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_from_data(data_df, size=5):\n",
    "    '''\n",
    "    Display grapheme images from sample data\n",
    "    param: data_df - sample of data\n",
    "    param: size - sqrt(sample size of data)\n",
    "    '''\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(size,size,figsize=(12,12))\n",
    "    # we show grapheme images for a selection of size x size samples\n",
    "    for i, index in enumerate(data_df.index):\n",
    "        image_id = data_df.iloc[i]['image_id']\n",
    "        flattened_image = data_df.iloc[i].drop('image_id').values.astype(np.uint8)\n",
    "        unpacked_image = PIL.Image.fromarray(flattened_image.reshape(137, 236))\n",
    "\n",
    "        ax[i//size, i%size].imshow(unpacked_image)\n",
    "        ax[i//size, i%size].set_title(image_id)\n",
    "        ax[i//size, i%size].axis('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_from_data(train_0_df.sample(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define another function to show the same grapheme in different writing, i.e. to perform sampling (based on variation of grapheme root, vowel diacritic and consonant diacritic, as parameters to the function) \\[1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_writting_variety(data_df=train_0_df, grapheme_root=72, vowel_diacritic=0,\\\n",
    "                             consonant_diacritic=0, size=5):\n",
    "    '''\n",
    "    This function get a set of grapheme root, vowel diacritic and consonant diacritic\n",
    "    and display a sample of 25 images for this grapheme\n",
    "    param: data_df - the dataset used as source of data\n",
    "    param: grapheme_root - the grapheme root label\n",
    "    param: vowel_diacritic - the vowel diacritic label\n",
    "    param: consonant_diacritic - the consonant diacritic label \n",
    "    param: size - sqrt(number of images to show)\n",
    "    '''\n",
    "    sample_train_df = train_df.loc[(train_df.grapheme_root == grapheme_root) & \\\n",
    "                                  (train_df.vowel_diacritic == vowel_diacritic) & \\\n",
    "                                  (train_df.consonant_diacritic == consonant_diacritic)]\n",
    "    print(f\"total: {sample_train_df.shape}\")\n",
    "    sample_df = data_df.merge(sample_train_df.image_id, how='inner')\n",
    "    print(f\"total: {sample_df.shape}\")\n",
    "    gr = sample_train_df.iloc[0]['grapheme']\n",
    "    cm_gr = class_map_df.loc[(class_map_df.component_type=='grapheme_root')& \\\n",
    "                             (class_map_df.label==grapheme_root), 'component'].values[0]\n",
    "    cm_vd = class_map_df.loc[(class_map_df.component_type=='vowel_diacritic')& \\\n",
    "                             (class_map_df.label==vowel_diacritic), 'component'].values[0]    \n",
    "    cm_cd = class_map_df.loc[(class_map_df.component_type=='consonant_diacritic')& \\\n",
    "                             (class_map_df.label==consonant_diacritic), 'component'].values[0]    \n",
    "    \n",
    "    print(f\"grapheme: {gr}, grapheme root: {cm_gr}, vowel discritic: {cm_vd}, consonant diacritic: {cm_cd}\")\n",
    "    sample_df = sample_df.sample(size * size)\n",
    "    display_image_from_data(sample_df, size=size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_writting_variety(train_0_df,72,1,1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[1\\] https://www.kaggle.com/gpreda/bengali-ai-handwritten-grapheme-getting-started Retrieved on: February 18, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "MLiP-group-SannedeKleijn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
