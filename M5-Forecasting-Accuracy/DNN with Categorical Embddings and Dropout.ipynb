{"cells":[{"cell_type":"code","source":"#team sanne de kleijn mlip, edit: tanh to relu\n\n\n# %% [markdown]\n# # M5 Forecast: Keras with Categorical Embeddings V2\n\n# %% [code]\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport os\nfrom tqdm.notebook import tqdm\n\n# %% [code]\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n# %% [markdown]\n# ## Load data\n\n# %% [code]\npath = \"../input/m5-forecasting-accuracy\"\n\ncalendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\nselling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\nsample_submission = pd.read_csv(os.path.join(path, \"sample_submission.csv\"))\n\n# %% [code]\nsales = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))\n\n# %% [markdown]\n# ## Describe and prepare data\n# \n# We will now go through all data sets and prepare them for modelling.\n\n# %% [markdown]\n# ### Calendar data\n# \n# For each date (covering both training and test data), we have access to useful calendar information.\n\n# %% [code]\ncalendar.head()\n\n# %% [code]\n\n# %% [code]\nfrom sklearn.preprocessing import OrdinalEncoder\n\ndef prep_calendar(df):\n    df = df.drop([\"date\", \"weekday\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    df = df.fillna(\"missing\")\n    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"})\n    df[cols] = OrdinalEncoder(dtype=\"int\").fit_transform(df[cols])\n    df = reduce_mem_usage(df)\n    return df\n\ncalendar = prep_calendar(calendar)\n\n# %% [code]\n\n# %% [markdown]\n# #### Notes for modeling\n# \n# **Features** deemed to be useful:\n# \n# - \"wday\", \"year\", \"month\" -> integer coding & embedding\n# - \"event_name_1\", \"event_type_1\" -> integer coding & embedding\n# - \"snap_XX\" -> numeric (they are dummies)\n# \n# **Reshape required**: No\n# \n# **Merge key(s)**: \"d\", \"wm_yr_wk\"\n\n# %% [markdown]\n# ### Selling prices\n# \n# Contains selling prices for each store_id, item_id_wm_yr_wk combination.\n\n# %% [code]\n# %% [markdown]\n# Derive some time related features:\n\n# %% [code]\ndef prep_selling_prices(df):\n    gr = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n    df[\"sell_price_rel_diff\"] = gr.pct_change()\n    df[\"sell_price_roll_sd7\"] = gr.transform(lambda x: x.rolling(7).std())\n    df[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) / (1 + gr.cummax() - gr.cummin())\n    df = reduce_mem_usage(df)\n    return df\n\nselling_prices = prep_selling_prices(selling_prices)\n\n\ndef reshape_sales(df, drop_d = None):\n    if drop_d is not None:\n        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n                 var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n    return df\n\nsales = reshape_sales(sales, 1000)\n\ndef prep_sales(df):\n    \n    df['lag_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(7))\n    df['lag_t28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    \n    df['rolling_mean_7_7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(7).rolling(7).mean())\n    df['rolling_mean_7_28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(7).rolling(28).mean())\n    \n    df['rolling_mean_28_7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    df['rolling_mean_28_28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(28).mean())\n    \n    df['rolling_median_28_7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).median())\n    df['rolling_median_28_28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(28).median())\n    \n    df['rolling_std_28_7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    df['rolling_std_28_28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(28).std())\n    \n    # Remove rows with NAs except for submission rows. rolling_mean_90_28 \n    df = df[(df.d >= 1914) | (pd.notna(df.rolling_mean_28_28))]# | (df.d <= 200) ]\n    df = reduce_mem_usage(df)\n\n    return df\n\n\n\nsales = sales.merge(calendar, how=\"left\", on=\"d\")\ngc.collect()\nsales.head()\n\n# %% [code]\nsales = sales.merge(selling_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\nsales.drop([\"wm_yr_wk\"], axis=1, inplace=True)\ngc.collect()\nsales.head()\n\n# %% [code]\nsales = prep_sales(sales)\n\ndel selling_prices\n\n# %% [markdown]\n# ## Prepare data for Keras interface\n\n# %% [markdown]\n# ### Ordinal encoding of remaining categoricals\n\n# %% [code]\ncat_id_cols = [\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\ncat_cols = cat_id_cols + [\"wday\", \"month\", \"year\", \"event_name_1\", \n                          \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n\n# In loop to minimize memory use\nfor i, v in tqdm(enumerate(cat_id_cols)):\n    sales[v] = OrdinalEncoder(dtype=\"int\").fit_transform(sales[[v]])\n\nsales = reduce_mem_usage(sales)\nsales.head()\ngc.collect()\n\n# %% [markdown]\n# #### Impute numeric columns\n\n# %% [code]\nnum_cols = [\"sell_price\", \"sell_price_rel_diff\", \"sell_price_roll_sd7\", \"sell_price_cumrel\",\n            \"lag_t28\", \n            \"rolling_mean_7_7\", \n            \"rolling_mean_7_28\",\n            \"rolling_mean_28_7\",\n            \"rolling_mean_28_28\",             \n            \"rolling_median_28_7\",\n            \"rolling_median_28_28\", \n            \"rolling_std_28_7\", \n            \"rolling_std_28_28\",\n           ]\nbool_cols = [\"snap_CA\", \"snap_TX\", \"snap_WI\"]\ndense_cols = num_cols + bool_cols\n\n# Need to do column by column due to memory constraints\nfor i, v in tqdm(enumerate(num_cols)):\n    sales[v] = sales[v].fillna(sales[v].median())\n    \nsales.head()\n\n# %% [markdown]\n# #### Separate submission data and reconstruct id columns\n\n# %% [code]\ntest = sales[sales.d >= 1914]\ntest = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n                   F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))\ntest.head()\ngc.collect()\n\ntest_long = sales[sales.d >= 1914 - 40]\ntest_long.head()\ngc.collect()\n\n# %% [markdown]\n# #### Make training data\n\n# %% [code]\n# Input dict for training with a dense array and separate inputs for each embedding input\ndef make_X(df):\n    X = {\"dense1\": df[dense_cols].to_numpy()}\n    for i, v in enumerate(cat_cols):\n        X[v] = df[[v]].to_numpy()\n    return X\n\n# Submission data\nX_test = make_X(test)\n\n# One month of validation data\nflag = (sales.d < 1914) & (sales.d >= 1914 - 28)\nvalid = (make_X(sales[flag]),\n         sales[\"demand\"][flag])\n\n# Rest is used for training\nflag = sales.d < 1914 #- 7\nX_train = make_X(sales[flag])\ny_train = sales[\"demand\"][flag]\n                             \ndel sales, flag\ngc.collect()\n\n\n# %% [code]\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Dropout, concatenate, Flatten, Dropout\nfrom tensorflow.keras.models import Model\n\n# %% [markdown]\n# ### Architecture with embeddings\n\n# %% [code]\ndef create_model(lr=0.002):\n    tf.keras.backend.clear_session()\n    gc.collect()\n\n    # Dense input\n    dense_input = Input(shape=(len(dense_cols), ), name='dense1')\n\n    # Embedding input\n    wday_input = Input(shape=(1,), name='wday')\n    month_input = Input(shape=(1,), name='month')\n    year_input = Input(shape=(1,), name='year')\n    event_name_1_input = Input(shape=(1,), name='event_name_1')\n    event_type_1_input = Input(shape=(1,), name='event_type_1')\n    event_name_2_input = Input(shape=(1,), name='event_name_2')\n    event_type_2_input = Input(shape=(1,), name='event_type_2')\n    item_id_input = Input(shape=(1,), name='item_id')\n    dept_id_input = Input(shape=(1,), name='dept_id')\n    store_id_input = Input(shape=(1,), name='store_id')\n    cat_id_input = Input(shape=(1,), name='cat_id')\n    state_id_input = Input(shape=(1,), name='state_id')\n\n    wday_emb = Flatten()(Embedding(7, 1)(wday_input))\n    month_emb = Flatten()(Embedding(12, 1)(month_input))\n    year_emb = Flatten()(Embedding(6, 1)(year_input))\n    event_name_1_emb = Flatten()(Embedding(31, 1)(event_name_1_input))\n    event_type_1_emb = Flatten()(Embedding(5, 1)(event_type_1_input))\n    event_name_2_emb = Flatten()(Embedding(5, 1)(event_name_2_input))\n    event_type_2_emb = Flatten()(Embedding(5, 1)(event_type_2_input))\n\n    item_id_emb = Flatten()(Embedding(3049, 3)(item_id_input))\n    dept_id_emb = Flatten()(Embedding(7, 1)(dept_id_input))\n    store_id_emb = Flatten()(Embedding(10, 1)(store_id_input))\n    cat_id_emb = Flatten()(Embedding(3, 1)(cat_id_input))\n    state_id_emb = Flatten()(Embedding(3, 1)(state_id_input))\n\n    # Combine dense and embedding parts and add dense layers. Exit on linear scale.\n    x = concatenate([dense_input, wday_emb, month_emb, year_emb, \n                     event_name_1_emb, event_type_1_emb, \n                     event_name_2_emb, event_type_2_emb, \n                     item_id_emb, dept_id_emb, store_id_emb,\n                     cat_id_emb, state_id_emb])\n    \n    x = Dropout(0.2)(x)\n    x = Dense(256, activation=\"tanh\")(x)\n    \n    x = Dropout(0.2)(x)\n    x = Dense(128, activation=\"tanh\")(x)\n    \n    x = Dropout(0.2)(x)\n    x = Dense(64, activation=\"tanh\")(x)\n    \n    x = Dropout(0.2)(x)\n    x = Dense(16, activation=\"tanh\")(x)\n    \n    x = Dropout(0.2)(x)\n    x = Dense(4, activation=\"tanh\")(x)\n    \n    x = Dropout(0.2)(x)\n    outputs = Dense(1, activation=\"linear\", name='output')(x)\n\n    inputs = {\"dense1\": dense_input, \"wday\": wday_input, \"month\": month_input, \"year\": year_input, \n              \"event_name_1\": event_name_1_input, \"event_type_1\": event_type_1_input,\n              \"event_name_2\": event_name_2_input, \"event_type_2\": event_type_2_input,\n              \"item_id\": item_id_input, \"dept_id\": dept_id_input, \"store_id\": store_id_input, \n              \"cat_id\": cat_id_input, \"state_id\": state_id_input}\n\n    # Connect input and output\n    model = Model(inputs, outputs)\n\n    model.compile(loss=keras.losses.mean_squared_error,\n                  metrics=[\"mse\"],\n                  optimizer=keras.optimizers.Adam(learning_rate=lr))\n    return model\n\n# %% [code]\nmodel = create_model(0.0002)\nmodel.summary()\nkeras.utils.plot_model(model, 'model.png', show_shapes=True)\n\n# %% [markdown]\n# ### Calculate derivatives and fit model\n\n# %% [code]\nhistory = model.fit(X_train, \n                    y_train,\n                    batch_size=2 ** 14,\n                    epochs=120,\n                    shuffle=True,\n                    validation_data=valid)\n\n# %% [markdown]\n# #### Plot the evaluation metrics over epochs\n\n# %% [code]\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.savefig('plt')\n\nplt.show()\n\n# %% [code]\nhistory.history[\"val_loss\"]\n\n# %% [code]\nmodel.save('model.h5')\n\n# %% [markdown]\n# ## Submission\n\nfor i in range(1914, 1969 +1):\n    print(i)\n    if i <= 1941:\n       if i >= 1921:\n            test_long['lag_t7'] = test_long.groupby(['id'])['demand'].transform(lambda x: x.shift(7))    \n            test_long['rolling_mean_7_7'] = test_long.groupby(['id'])['demand'].transform(lambda x: x.shift(7).rolling(7).mean())\n            test_long['rolling_mean_7_28'] = test_long.groupby(['id'])['demand'].transform(lambda x: x.shift(7).rolling(28).mean())\n            \n       forecast = make_X(test_long[test_long.d == i])\n       pred = model.predict(forecast, batch_size=2 ** 14)\n\n       test_long.loc[test_long.d == i, \"demand\"] = pred.clip(0) * 1.02\n    else:\n        test_long.loc[test_long.d == i, \"demand\"] = 0\n\n\ntest = test_long[test_long.d >= 1914]\ntest = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n                   F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))\nsubmission = test.pivot(index=\"id\", columns=\"F\", values=\"demand\").reset_index()[sample_submission.columns]\nsubmission = sample_submission[[\"id\"]].merge(submission, how=\"left\", on=\"id\")\nsubmission.head()\n\n# %% [code]\n# %% [code]\nsubmission.to_csv(\"dnn_fake_valid_day_to_day.csv\", index=False)","metadata":{"collapsed":false,"_kg_hide-input":false},"execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}